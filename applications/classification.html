
<!DOCTYPE html>

<html>
  <head>
    <meta charset="utf-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" />
    <title>Classification &#8212; QuantEcon DataScience</title>
    
  <link href="../_static/css/theme.css" rel="stylesheet" />
  <link href="../_static/css/index.c5995385ac14fb8791e8eb36b4908be2.css" rel="stylesheet" />

    
  <link rel="stylesheet"
    href="../_static/vendor/fontawesome/5.13.0/css/all.min.css">
  <link rel="preload" as="font" type="font/woff2" crossorigin
    href="../_static/vendor/fontawesome/5.13.0/webfonts/fa-solid-900.woff2">
  <link rel="preload" as="font" type="font/woff2" crossorigin
    href="../_static/vendor/fontawesome/5.13.0/webfonts/fa-brands-400.woff2">

    
      

    
    <link rel="stylesheet" href="../_static/pygments.css" type="text/css" />
    <link rel="stylesheet" href="../_static/sphinx-book-theme.acff12b8f9c144ce68a297486a2fa670.css" type="text/css" />
    <link rel="stylesheet" type="text/css" href="../_static/togglebutton.css" />
    <link rel="stylesheet" type="text/css" href="../_static/copybutton.css" />
    <link rel="stylesheet" type="text/css" href="../_static/mystnb.css" />
    <link rel="stylesheet" type="text/css" href="../_static/sphinx-thebe.css" />
    <link rel="stylesheet" type="text/css" href="../_static/exercise.css" />
    <link rel="stylesheet" type="text/css" href="../_static/panels-main.c949a650a448cc0ae9fd3441c0e17fb0.css" />
    <link rel="stylesheet" type="text/css" href="../_static/panels-variables.06eb56fa6e07937060861dad626602ad.css" />
    
  <link rel="preload" as="script" href="../_static/js/index.1c5a1a01449ed65a7b51.js">

    <script id="documentation_options" data-url_root="../" src="../_static/documentation_options.js"></script>
    <script src="../_static/jquery.js"></script>
    <script src="../_static/underscore.js"></script>
    <script src="../_static/doctools.js"></script>
    <script src="../_static/togglebutton.js"></script>
    <script src="../_static/clipboard.min.js"></script>
    <script src="../_static/copybutton.js"></script>
    <script >var togglebuttonSelector = '.toggle, .admonition.dropdown, .tag_hide_input div.cell_input, .tag_hide-input div.cell_input, .tag_hide_output div.cell_output, .tag_hide-output div.cell_output, .tag_hide_cell.cell, .tag_hide-cell.cell';</script>
    <script src="../_static/sphinx-book-theme.12a9622fbb08dcb3a2a40b2c02b83a57.js"></script>
    <script src="https://cdnjs.cloudflare.com/ajax/libs/require.js/2.3.4/require.min.js"></script>
    <script src="https://unpkg.com/@jupyter-widgets/html-manager@^0.20.0/dist/embed-amd.js"></script>
    <script async="async" src="https://unpkg.com/thebelab@latest/lib/index.js"></script>
    <script >
        const thebe_selector = ".thebe"
        const thebe_selector_input = "pre"
        const thebe_selector_output = ".output"
    </script>
    <script async="async" src="../_static/sphinx-thebe.js"></script>
    <script async="async" src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.7/latest.js?config=TeX-AMS-MML_HTMLorMML"></script>
    <script type="text/x-mathjax-config">MathJax.Hub.Config({"tex2jax": {"inlineMath": [["\\(", "\\)"]], "displayMath": [["\\[", "\\]"]], "processRefs": false, "processEnvironments": false}})</script>
    <link rel="canonical" href="https://datascience.quantecon.org/applications/classification.html" />
    <link rel="shortcut icon" href="../_static/lectures-favicon.ico"/>
    <link rel="index" title="Index" href="../genindex.html" />
    <link rel="search" title="Search" href="../search.html" />
    <link rel="next" title="Working with Text" href="working_with_text.html" />
    <link rel="prev" title="Mapping in Python" href="maps.html" />
    <meta name="viewport" content="width=device-width, initial-scale=1" />
    <meta name="docsearch:language" content="en" />
    
  </head>
  <body data-spy="scroll" data-target="#bd-toc-nav" data-offset="80">
    
    <div class="container-fluid" id="banner"></div>

    

    <div class="container-xl">
      <div class="row">
          
<div class="col-12 col-md-3 bd-sidebar site-navigation show" id="site-navigation">
    
        <div class="navbar-brand-box">
    <a class="navbar-brand text-wrap" href="../index.html">
      
      <img src="../_static/datascience-logo.png" class="logo" alt="logo">
      
      
      <h1 class="site-logo" id="site-title">QuantEcon DataScience</h1>
      
    </a>
</div><form class="bd-search d-flex align-items-center" action="../search.html" method="get">
  <i class="icon fas fa-search"></i>
  <input type="search" class="form-control" name="q" id="search-input" placeholder="Search the docs ..." aria-label="Search the docs ..." autocomplete="off" >
</form><nav class="bd-links" id="bd-docs-nav" aria-label="Main navigation">
    <div class="bd-toc-item active">
        <ul class="current nav bd-sidenav">
 <li class="toctree-l1 has-children">
  <a class="reference internal" href="../introduction/index.html">
   Introduction
  </a>
  <input class="toctree-checkbox" id="toctree-checkbox-1" name="toctree-checkbox-1" type="checkbox"/>
  <label for="toctree-checkbox-1">
   <i class="fas fa-chevron-down">
   </i>
  </label>
  <ul>
   <li class="toctree-l2">
    <a class="reference internal" href="../introduction/overview.html">
     Course Description
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../introduction/getting_started.html">
     Getting Started
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../introduction/cloud_setup.html">
     Cloud Setup
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../introduction/local_install.html">
     Local Installation
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../introduction/troubleshooting.html">
     Troubleshooting
    </a>
   </li>
  </ul>
 </li>
 <li class="toctree-l1 has-children">
  <a class="reference internal" href="../python_fundamentals/index.html">
   Python Fundamentals
  </a>
  <input class="toctree-checkbox" id="toctree-checkbox-2" name="toctree-checkbox-2" type="checkbox"/>
  <label for="toctree-checkbox-2">
   <i class="fas fa-chevron-down">
   </i>
  </label>
  <ul>
   <li class="toctree-l2">
    <a class="reference internal" href="../python_fundamentals/basics.html">
     Basics
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../python_fundamentals/collections.html">
     Collections
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../python_fundamentals/control_flow.html">
     Control Flow
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../python_fundamentals/functions.html">
     Functions
    </a>
   </li>
  </ul>
 </li>
 <li class="toctree-l1 has-children">
  <a class="reference internal" href="../scientific/index.html">
   Scientific Computing
  </a>
  <input class="toctree-checkbox" id="toctree-checkbox-3" name="toctree-checkbox-3" type="checkbox"/>
  <label for="toctree-checkbox-3">
   <i class="fas fa-chevron-down">
   </i>
  </label>
  <ul>
   <li class="toctree-l2">
    <a class="reference internal" href="../scientific/numpy_arrays.html">
     Introduction to Numpy
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../scientific/plotting.html">
     Plotting
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../scientific/applied_linalg.html">
     Applied Linear Algebra
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../scientific/randomness.html">
     Randomness
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../scientific/optimization.html">
     Optimization
    </a>
   </li>
  </ul>
 </li>
 <li class="toctree-l1 has-children">
  <a class="reference internal" href="../pandas/index.html">
   pandas
  </a>
  <input class="toctree-checkbox" id="toctree-checkbox-4" name="toctree-checkbox-4" type="checkbox"/>
  <label for="toctree-checkbox-4">
   <i class="fas fa-chevron-down">
   </i>
  </label>
  <ul>
   <li class="toctree-l2">
    <a class="reference internal" href="../pandas/intro.html">
     Introduction
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../pandas/basics.html">
     Basic Functionality
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../pandas/the_index.html">
     The Index
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../pandas/storage_formats.html">
     Storage Formats
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../pandas/data_clean.html">
     Cleaning Data
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../pandas/reshape.html">
     Reshape
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../pandas/merge.html">
     Merge
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../pandas/groupby.html">
     GroupBy
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../pandas/timeseries.html">
     Time series
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../pandas/matplotlib.html">
     Intermediate Plotting
    </a>
   </li>
  </ul>
 </li>
 <li class="toctree-l1 current active has-children">
  <a class="reference internal" href="index.html">
   Applications
  </a>
  <input checked="" class="toctree-checkbox" id="toctree-checkbox-5" name="toctree-checkbox-5" type="checkbox"/>
  <label for="toctree-checkbox-5">
   <i class="fas fa-chevron-down">
   </i>
  </label>
  <ul class="current">
   <li class="toctree-l2">
    <a class="reference internal" href="visualization_rules.html">
     Data Visualization: Rules and Guidelines
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="regression.html">
     Regression
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="recidivism.html">
     Case Study: Recidivism
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="maps.html">
     Mapping in Python
    </a>
   </li>
   <li class="toctree-l2 current active">
    <a class="current reference internal" href="#">
     Classification
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="working_with_text.html">
     Working with Text
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="ml_in_economics.html">
     Machine Learning in Economics
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="heterogeneity.html">
     Heterogeneous Effects
    </a>
   </li>
  </ul>
 </li>
</ul>

    </div>
</nav> <!-- To handle the deprecated key -->

<div class="navbar_extra_footer">
  Theme by the <a href="https://ebp.jupyterbook.org">Executable Book Project</a>
</div>

</div>


          


          
<main class="col py-md-3 pl-md-4 bd-content overflow-auto" role="main">
    
    <div class="topbar container-xl fixed-top">
    <div class="topbar-contents row">
        <div class="col-12 col-md-3 bd-topbar-whitespace site-navigation show"></div>
        <div class="col pl-md-4 topbar-main">
            
            <button id="navbar-toggler" class="navbar-toggler ml-0" type="button" data-toggle="collapse"
                data-toggle="tooltip" data-placement="bottom" data-target=".site-navigation" aria-controls="navbar-menu"
                aria-expanded="true" aria-label="Toggle navigation" aria-controls="site-navigation"
                title="Toggle navigation" data-toggle="tooltip" data-placement="left">
                <i class="fas fa-bars"></i>
                <i class="fas fa-arrow-left"></i>
                <i class="fas fa-arrow-up"></i>
            </button>
            
            
<div class="dropdown-buttons-trigger">
    <button id="dropdown-buttons-trigger" class="btn btn-secondary topbarbtn" aria-label="Download this page"><i
            class="fas fa-download"></i></button>

    <div class="dropdown-buttons">
        <!-- ipynb file if we had a myst markdown file -->
        
        <!-- Download raw file -->
        <a class="dropdown-buttons" href="../_sources/applications/classification.md"><button type="button"
                class="btn btn-secondary topbarbtn" title="Download source file" data-toggle="tooltip"
                data-placement="left">.md</button></a>
        <!-- Download PDF via print -->
        <button type="button" id="download-print" class="btn btn-secondary topbarbtn" title="Print to PDF"
            onClick="window.print()" data-toggle="tooltip" data-placement="left">.pdf</button>
    </div>
</div>

            <!-- Source interaction buttons -->

            <!-- Full screen (wrap in <a> to have style consistency -->

<a class="full-screen-button"><button type="button" class="btn btn-secondary topbarbtn" data-toggle="tooltip"
        data-placement="bottom" onclick="toggleFullScreen()" aria-label="Fullscreen mode"
        title="Fullscreen mode"><i
            class="fas fa-expand"></i></button></a>

            <!-- Launch buttons -->

        </div>

        <!-- Table of contents -->
        <div class="d-none d-md-block col-md-2 bd-toc show">
            
            <div class="tocsection onthispage pt-5 pb-3">
                <i class="fas fa-list"></i> Contents
            </div>
            <nav id="bd-toc-nav">
                <ul class="visible nav section-nav flex-column">
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#introduction-to-classification">
   Introduction to Classification
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#warmup-example-logistic-regression">
   Warmup Example: Logistic Regression
  </a>
  <ul class="nav section-nav flex-column">
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#visualization-decision-boundaries">
     Visualization: Decision Boundaries
    </a>
   </li>
  </ul>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#model-evaluation">
   Model Evaluation
  </a>
  <ul class="nav section-nav flex-column">
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#accuracy">
     Accuracy
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#precision-and-recall">
     Precision and Recall
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#roc-and-auc">
     ROC and AUC
    </a>
   </li>
  </ul>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#neural-network-classifiers">
   Neural Network Classifiers
  </a>
  <ul class="nav section-nav flex-column">
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#aside-neural-network-toolboxes">
     Aside: Neural Network Toolboxes
    </a>
   </li>
  </ul>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#application-predicting-us-recessions">
   Application: Predicting US Recessions
  </a>
  <ul class="nav section-nav flex-column">
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#data-prep">
     Data Prep
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#how-many-leads">
     How Many leads?
    </a>
   </li>
  </ul>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#exercises">
   Exercises
  </a>
  <ul class="nav section-nav flex-column">
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#exercise-1">
     Exercise 1
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#exercise-2">
     Exercise 2
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#exercise-3">
     Exercise 3
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#exercise-4">
     Exercise 4
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#exercise-5">
     Exercise 5
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#exercise-6">
     Exercise 6
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#exercise-7">
     Exercise 7
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#exercise-8">
     Exercise 8
    </a>
   </li>
  </ul>
 </li>
</ul>

            </nav>
        </div>
    </div>
</div>
    <div id="main-content" class="row">
        <div class="col-12 col-md-9 pl-md-3 pr-md-0">
        
              <div>
                
  <div class="section" id="classification">
<h1>Classification<a class="headerlink" href="#classification" title="Permalink to this headline">¶</a></h1>
<p><strong>Co-authors</strong></p>
<blockquote>
<div><ul class="simple">
<li><p><a class="reference external" href="https://github.com/QBatista">Quentin Batista <em>University of Tokyo</em></a></p></li>
<li><p><a class="reference external" href="http://www.tomsargent.com/">Thomas Sargent <em>NYU</em></a></p></li>
<li><p><a class="reference external" href="https://economics.ubc.ca/faculty-and-staff/paul-schrimpf/">Paul Schrimpf <em>UBC</em></a></p></li>
<li><p><a class="reference external" href="https://github.com/natashawatkins">Natasha Watkins <em>UCLA</em></a></p></li>
</ul>
</div></blockquote>
<p><strong>Prerequisites</strong></p>
<ul class="simple">
<li><p><a class="reference internal" href="regression.html"><span class="doc">Regression</span></a></p></li>
</ul>
<p><strong>Outcomes</strong></p>
<ul class="simple">
<li><p>Understand what problems classification solves</p></li>
<li><p>Evaluate classification models using a variety of metrics</p></li>
</ul>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="c1"># Uncomment following line to install on colab</span>
<span class="c1">#! pip install qeds fiona geopandas xgboost gensim folium pyLDAvis descartes</span>
</pre></div>
</div>
<div class="section" id="introduction-to-classification">
<h2>Introduction to Classification<a class="headerlink" href="#introduction-to-classification" title="Permalink to this headline">¶</a></h2>
<p>We now move from regression to the second main branch of machine learning:
classification.</p>
<p>Recall that the regression problem mapped a set of
feature variables to a continuous target.</p>
<p>Classification is similar to regression, but instead of predicting a continuous
target, classification algorithms attempt to apply one (or more) of a discrete
number of labels or classes to each observation.</p>
<p>Another perspective is that for regression, the targets are usually
continuous-valued, while in classification, the targets are categorical.</p>
<p>Common examples of classification problems are</p>
<ul class="simple">
<li><p>Labeling emails as spam or not spam</p></li>
<li><p>Person identification in a photo</p></li>
<li><p>Speech recognition</p></li>
<li><p>Whether or not a country is or will be in a recession</p></li>
</ul>
<p>Classification can also be applied in settings where the target isn’t naturally
categorical.</p>
<p>For example, suppose we want to predict whether the unemployment rate for a state
will be low (<span class="math notranslate nohighlight">\(&lt;3\%\)</span>), medium (<span class="math notranslate nohighlight">\(\in [3\%, 5\%]\)</span>), or high (<span class="math notranslate nohighlight">\(&gt;5\%\)</span>)
but don’t care about the actual number.</p>
<p>Most economic problems are posed in continuous terms, so it may take some creativity
to determine the optimal way to categorize a target variable so
classification algorithms can be applied.</p>
<p>As many problems can be posed either as classification or regression, many
machine learning algorithms have variants that perform regression or
classification tasks.</p>
<p>Throughout this lecture, we will revisit some of the algorithms from the
<a class="reference internal" href="regression.html"><span class="doc">regression</span></a> lecture and discuss how they can be applied in
classification settings.</p>
<p>As we have already seen relatives of these algorithms, this lecture will be
lighter on exposition and then build up to an application.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">datetime</span>
<span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="nn">np</span>
<span class="kn">import</span> <span class="nn">pandas</span> <span class="k">as</span> <span class="nn">pd</span>
<span class="kn">import</span> <span class="nn">seaborn</span> <span class="k">as</span> <span class="nn">sns</span>
<span class="kn">import</span> <span class="nn">pandas_datareader.data</span> <span class="k">as</span> <span class="nn">web</span>

<span class="kn">from</span> <span class="nn">sklearn</span> <span class="kn">import</span> <span class="p">(</span>
    <span class="n">linear_model</span><span class="p">,</span> <span class="n">metrics</span><span class="p">,</span> <span class="n">neural_network</span><span class="p">,</span> <span class="n">pipeline</span><span class="p">,</span> <span class="n">preprocessing</span><span class="p">,</span> <span class="n">model_selection</span>
<span class="p">)</span>

<span class="kn">import</span> <span class="nn">matplotlib.pyplot</span> <span class="k">as</span> <span class="nn">plt</span>
<span class="o">%</span><span class="k">matplotlib</span> inline
<span class="c1"># activate plot theme</span>
<span class="kn">import</span> <span class="nn">qeds</span>
<span class="n">qeds</span><span class="o">.</span><span class="n">themes</span><span class="o">.</span><span class="n">mpl_style</span><span class="p">();</span>
</pre></div>
</div>
</div>
</div>
</div>
<div class="section" id="warmup-example-logistic-regression">
<h2>Warmup Example: Logistic Regression<a class="headerlink" href="#warmup-example-logistic-regression" title="Permalink to this headline">¶</a></h2>
<p>We have actually already encountered a classification algorithm.</p>
<p>In the <a class="reference internal" href="recidivism.html"><span class="doc">recidivism</span></a> example, we attempted to predict whether
or not an individual would commit another crime by using a combination of the
assigned COMPAS score and the individual’s gender or race.</p>
<p>In that example, we used a <em>logistic regression</em> model, which is a close
relative of the linear regression model from the <a class="reference internal" href="regression.html"><span class="doc">regression</span></a> section.</p>
<p>The logistic regression model for predicting the likelihood of recidivism using
the <code class="docutils literal notranslate"><span class="pre">COMPAS</span></code> score as the single feature is written</p>
<div class="math notranslate nohighlight">
\[
p(\text{recid}) = L(\beta_0 + \beta_1 \text{COMPAS} + \epsilon)
\]</div>
<p>where <span class="math notranslate nohighlight">\(L\)</span> is the <em>logistic function</em>: <span class="math notranslate nohighlight">\(L(x) = \frac{1}{1 + e^{-x}}\)</span>.</p>
<p>To get some intuition for this function, let’s plot it below.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">x</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">linspace</span><span class="p">(</span><span class="o">-</span><span class="mi">5</span><span class="p">,</span> <span class="mi">5</span><span class="p">,</span> <span class="mi">100</span><span class="p">)</span>
<span class="n">y</span> <span class="o">=</span> <span class="mi">1</span><span class="o">/</span><span class="p">(</span><span class="mi">1</span><span class="o">+</span><span class="n">np</span><span class="o">.</span><span class="n">exp</span><span class="p">(</span><span class="o">-</span><span class="n">x</span><span class="p">))</span>
<span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">y</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>[&lt;matplotlib.lines.Line2D at 0x7f55eec41c70&gt;]
</pre></div>
</div>
<img alt="../_images/classification_3_1.png" src="../_images/classification_3_1.png" />
</div>
</div>
<p>Notice that for all values of <span class="math notranslate nohighlight">\(x\)</span>, the value of the logistic function is
always between 0 and 1.</p>
<p>This is perfect for binary classification problems that need to
output the probability of one of the two labels.</p>
<p>Let’s load up the recidivism data and fit the logistic regression model.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">data_url</span> <span class="o">=</span> <span class="s2">&quot;https://raw.githubusercontent.com/propublica/compas-analysis&quot;</span>
<span class="n">data_url</span> <span class="o">+=</span> <span class="s2">&quot;/master/compas-scores-two-years.csv&quot;</span>

<span class="n">df</span> <span class="o">=</span> <span class="n">pd</span><span class="o">.</span><span class="n">read_csv</span><span class="p">(</span><span class="n">data_url</span><span class="p">)</span>
<span class="n">df</span><span class="o">.</span><span class="n">head</span><span class="p">()</span>

<span class="n">X</span> <span class="o">=</span> <span class="n">df</span><span class="p">[[</span><span class="s2">&quot;decile_score&quot;</span><span class="p">]]</span>
<span class="n">y</span> <span class="o">=</span> <span class="n">df</span><span class="p">[</span><span class="s2">&quot;two_year_recid&quot;</span><span class="p">]</span>
<span class="n">X_train</span><span class="p">,</span> <span class="n">X_test</span><span class="p">,</span> <span class="n">y_train</span><span class="p">,</span> <span class="n">y_test</span> <span class="o">=</span> <span class="n">model_selection</span><span class="o">.</span><span class="n">train_test_split</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">,</span> <span class="n">test_size</span><span class="o">=</span><span class="mf">0.25</span><span class="p">)</span>

<span class="n">logistic_model</span> <span class="o">=</span> <span class="n">linear_model</span><span class="o">.</span><span class="n">LogisticRegression</span><span class="p">(</span><span class="n">solver</span><span class="o">=</span><span class="s2">&quot;lbfgs&quot;</span><span class="p">)</span>
<span class="n">logistic_model</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X_train</span><span class="p">,</span> <span class="n">y_train</span><span class="p">)</span>

<span class="n">beta_0</span> <span class="o">=</span> <span class="n">logistic_model</span><span class="o">.</span><span class="n">intercept_</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span>
<span class="n">beta_1</span> <span class="o">=</span> <span class="n">logistic_model</span><span class="o">.</span><span class="n">coef_</span><span class="p">[</span><span class="mi">0</span><span class="p">][</span><span class="mi">0</span><span class="p">]</span>

<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Fit model: p(recid) = L(</span><span class="si">{</span><span class="n">beta_0</span><span class="si">:</span><span class="s2">.4f</span><span class="si">}</span><span class="s2"> + </span><span class="si">{</span><span class="n">beta_1</span><span class="si">:</span><span class="s2">.4f</span><span class="si">}</span><span class="s2"> decile_score)&quot;</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Fit model: p(recid) = L(-1.4191 + 0.2663 decile_score)
</pre></div>
</div>
</div>
</div>
<p>From these coefficients, we see that an increase in the <code class="docutils literal notranslate"><span class="pre">decile_score</span></code> leads
to an increase in the predicted probability of recidivism.</p>
<p>Suppose we choose to classify any model output greater than 0.5 as “at risk of
recidivism”.</p>
<p>Then, the positive coefficient on <code class="docutils literal notranslate"><span class="pre">decile_score</span></code> means that there is some cutoff score above which all individuals will be labeled as high-risk.</p>
<div class="admonition-exercise admonition" id="app-cls-dir1">
<p class="admonition-title">Exercise</p>
<p>See exercise 1 in the <a class="reference internal" href="#app-cls-ex"><span class="std std-ref">exercise list</span></a>.</p>
</div>
<div class="section" id="visualization-decision-boundaries">
<h3>Visualization: Decision Boundaries<a class="headerlink" href="#visualization-decision-boundaries" title="Permalink to this headline">¶</a></h3>
<p>With just one feature that has a positive coefficient, the model’s predictions
will always have this cutoff structure.</p>
<p>Let’s add a second feature the model: the age of the individual.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">X</span> <span class="o">=</span> <span class="n">df</span><span class="p">[[</span><span class="s2">&quot;decile_score&quot;</span><span class="p">,</span> <span class="s2">&quot;age&quot;</span><span class="p">]]</span>
<span class="n">X_train</span><span class="p">,</span> <span class="n">X_test</span><span class="p">,</span> <span class="n">y_train</span><span class="p">,</span> <span class="n">y_test</span> <span class="o">=</span> <span class="n">model_selection</span><span class="o">.</span><span class="n">train_test_split</span><span class="p">(</span>
    <span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">,</span> <span class="n">test_size</span><span class="o">=</span><span class="mf">0.25</span><span class="p">,</span> <span class="n">random_state</span><span class="o">=</span><span class="mi">42</span>
<span class="p">)</span>

<span class="n">logistic_age_model</span> <span class="o">=</span> <span class="n">linear_model</span><span class="o">.</span><span class="n">LogisticRegression</span><span class="p">(</span><span class="n">solver</span><span class="o">=</span><span class="s2">&quot;lbfgs&quot;</span><span class="p">)</span>
<span class="n">logistic_age_model</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X_train</span><span class="p">,</span> <span class="n">y_train</span><span class="p">)</span>

<span class="n">beta_0</span> <span class="o">=</span> <span class="n">logistic_age_model</span><span class="o">.</span><span class="n">intercept_</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span>
<span class="n">beta_1</span><span class="p">,</span> <span class="n">beta_2</span> <span class="o">=</span> <span class="n">logistic_age_model</span><span class="o">.</span><span class="n">coef_</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span>

<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Fit model: p(recid) = L(</span><span class="si">{</span><span class="n">beta_0</span><span class="si">:</span><span class="s2">.4f</span><span class="si">}</span><span class="s2"> + </span><span class="si">{</span><span class="n">beta_1</span><span class="si">:</span><span class="s2">.4f</span><span class="si">}</span><span class="s2"> decile_score + </span><span class="si">{</span><span class="n">beta_2</span><span class="si">:</span><span class="s2">.4f</span><span class="si">}</span><span class="s2"> age)&quot;</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Fit model: p(recid) = L(-0.8505 + 0.2470 decile_score + -0.0130 age)
</pre></div>
</div>
</div>
</div>
<p>Here, we see that an increase in the <code class="docutils literal notranslate"><span class="pre">decile_score</span></code> still leads to an increase in
the predicted probability of recidivism, while older individuals are slightly
less likely to commit crime again.</p>
<p>We’ll build on an example from the <a class="reference external" href="https://scikit-learn.org/stable/auto_examples/svm/plot_iris.html">scikit-learn documentation</a> to visualize the predictions of this model.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="k">def</span> <span class="nf">plot_contours</span><span class="p">(</span><span class="n">ax</span><span class="p">,</span> <span class="n">mod</span><span class="p">,</span> <span class="n">xx</span><span class="p">,</span> <span class="n">yy</span><span class="p">,</span> <span class="o">**</span><span class="n">params</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Plot the decision boundaries for a classifier with 2 features x and y.</span>

<span class="sd">    Parameters</span>
<span class="sd">    ----------</span>
<span class="sd">    ax: matplotlib axes object</span>
<span class="sd">    mod: a classifier</span>
<span class="sd">    xx: meshgrid ndarray</span>
<span class="sd">    yy: meshgrid ndarray</span>
<span class="sd">    params: dictionary of params to pass to contourf, optional</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">Z</span> <span class="o">=</span> <span class="n">mod</span><span class="o">.</span><span class="n">predict</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">c_</span><span class="p">[</span><span class="n">xx</span><span class="o">.</span><span class="n">ravel</span><span class="p">(),</span> <span class="n">yy</span><span class="o">.</span><span class="n">ravel</span><span class="p">()])</span>
    <span class="n">Z</span> <span class="o">=</span> <span class="n">Z</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="n">xx</span><span class="o">.</span><span class="n">shape</span><span class="p">)</span>
    <span class="n">out</span> <span class="o">=</span> <span class="n">ax</span><span class="o">.</span><span class="n">contourf</span><span class="p">(</span><span class="n">xx</span><span class="p">,</span> <span class="n">yy</span><span class="p">,</span> <span class="n">Z</span><span class="p">,</span> <span class="o">**</span><span class="n">params</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">out</span>

<span class="k">def</span> <span class="nf">fit_and_plot_decision_boundary</span><span class="p">(</span><span class="n">mod</span><span class="p">,</span> <span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">,</span> <span class="o">**</span><span class="n">params</span><span class="p">):</span>
    <span class="c1"># fit model</span>
    <span class="n">mod</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">)</span>

    <span class="c1"># generate grids of first two columns of X</span>
    <span class="k">def</span> <span class="nf">gen_grid</span><span class="p">(</span><span class="n">xseries</span><span class="p">):</span>
        <span class="k">if</span> <span class="n">xseries</span><span class="o">.</span><span class="n">nunique</span><span class="p">()</span> <span class="o">&lt;</span> <span class="mi">50</span><span class="p">:</span>
            <span class="k">return</span> <span class="nb">sorted</span><span class="p">(</span><span class="n">xseries</span><span class="o">.</span><span class="n">unique</span><span class="p">())</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="k">return</span> <span class="n">np</span><span class="o">.</span><span class="n">linspace</span><span class="p">(</span><span class="n">xseries</span><span class="o">.</span><span class="n">min</span><span class="p">(),</span> <span class="n">xseries</span><span class="o">.</span><span class="n">max</span><span class="p">(),</span> <span class="mi">50</span><span class="p">)</span>

    <span class="n">x1</span><span class="p">,</span> <span class="n">x2</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">meshgrid</span><span class="p">(</span><span class="n">gen_grid</span><span class="p">(</span><span class="n">X</span><span class="o">.</span><span class="n">iloc</span><span class="p">[:,</span> <span class="mi">0</span><span class="p">]),</span> <span class="n">gen_grid</span><span class="p">(</span><span class="n">X</span><span class="o">.</span><span class="n">iloc</span><span class="p">[:,</span> <span class="mi">1</span><span class="p">]))</span>

    <span class="c1"># plot contours and scatter</span>
    <span class="n">fig</span><span class="p">,</span> <span class="n">ax</span> <span class="o">=</span> <span class="n">plt</span><span class="o">.</span><span class="n">subplots</span><span class="p">()</span>
    <span class="n">plot_contours</span><span class="p">(</span><span class="n">ax</span><span class="p">,</span> <span class="n">mod</span><span class="p">,</span> <span class="n">x1</span><span class="p">,</span> <span class="n">x2</span><span class="p">,</span> <span class="o">**</span><span class="n">params</span><span class="p">)</span>
    <span class="n">x1_name</span><span class="p">,</span> <span class="n">x2_name</span> <span class="o">=</span> <span class="nb">list</span><span class="p">(</span><span class="n">X</span><span class="p">)[:</span><span class="mi">2</span><span class="p">]</span>
    <span class="n">X</span><span class="o">.</span><span class="n">plot</span><span class="o">.</span><span class="n">scatter</span><span class="p">(</span><span class="n">x</span><span class="o">=</span><span class="n">x1_name</span><span class="p">,</span> <span class="n">y</span><span class="o">=</span><span class="n">x2_name</span><span class="p">,</span> <span class="n">color</span><span class="o">=</span><span class="n">y</span><span class="p">,</span> <span class="n">ax</span><span class="o">=</span><span class="n">ax</span><span class="p">)</span>
    <span class="n">ax</span><span class="o">.</span><span class="n">set_xlabel</span><span class="p">(</span><span class="n">x1_name</span><span class="p">)</span>
    <span class="n">ax</span><span class="o">.</span><span class="n">set_ylabel</span><span class="p">(</span><span class="n">x2_name</span><span class="p">)</span>

    <span class="k">return</span> <span class="n">ax</span>

<span class="n">fit_and_plot_decision_boundary</span><span class="p">(</span>
    <span class="n">linear_model</span><span class="o">.</span><span class="n">LogisticRegression</span><span class="p">(</span><span class="n">solver</span><span class="o">=</span><span class="s2">&quot;lbfgs&quot;</span><span class="p">),</span>
    <span class="n">X_train</span><span class="p">,</span> <span class="n">y_train</span><span class="p">,</span> <span class="n">cmap</span><span class="o">=</span><span class="n">plt</span><span class="o">.</span><span class="n">cm</span><span class="o">.</span><span class="n">Greys</span>
<span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>&lt;AxesSubplot:xlabel=&#39;decile_score&#39;, ylabel=&#39;age&#39;&gt;
</pre></div>
</div>
<img alt="../_images/classification_9_1.png" src="../_images/classification_9_1.png" />
</div>
</div>
<p>In this plot, we can clearly see the relationships we identified from the
coefficients.</p>
<p>However, we do see that the model is not perfect, as some solid circles are
in the light section and some light circles in the solid section.</p>
<p>This is likely caused by two things:</p>
<ol class="simple">
<li><p>The model inside the logistic function is a linear regression – thus only a
linear combination of the input features can be used for prediction.</p></li>
<li><p>Drawing a straight line (linear) that perfectly separates
true observations from the false is impossible.</p></li>
</ol>
<div class="admonition-exercise admonition" id="app-cls-dir2">
<p class="admonition-title">Exercise</p>
<p>See exercise 2 in the <a class="reference internal" href="#app-cls-ex"><span class="std std-ref">exercise list</span></a>.</p>
</div>
</div>
</div>
<div class="section" id="model-evaluation">
<h2>Model Evaluation<a class="headerlink" href="#model-evaluation" title="Permalink to this headline">¶</a></h2>
<p>Before we get too far into additional classification algorithms, let’s take a
step back and think about how to evaluate the performance of a classification
model.</p>
<div class="section" id="accuracy">
<h3>Accuracy<a class="headerlink" href="#accuracy" title="Permalink to this headline">¶</a></h3>
<p>Perhaps the most intuitive classification metric is <em>accuracy</em>, which is the
fraction of correct predictions.</p>
<p>For a scikit-learn classifier, this can be computed using the <code class="docutils literal notranslate"><span class="pre">score</span></code> method.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">train_acc</span> <span class="o">=</span> <span class="n">logistic_age_model</span><span class="o">.</span><span class="n">score</span><span class="p">(</span><span class="n">X_train</span><span class="p">,</span> <span class="n">y_train</span><span class="p">)</span>
<span class="n">test_acc</span> <span class="o">=</span> <span class="n">logistic_age_model</span><span class="o">.</span><span class="n">score</span><span class="p">(</span><span class="n">X_test</span><span class="p">,</span> <span class="n">y_test</span><span class="p">)</span>

<span class="n">train_acc</span><span class="p">,</span> <span class="n">test_acc</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>(0.6534195933456562, 0.667960088691796)
</pre></div>
</div>
</div>
</div>
<p>When the testing accuracy is similar to or higher than the training
accuracy (as it is here), the model might be underfitting.
Thus, we should consider either using a more powerful model or adding additional
features.</p>
<p>In many contexts, this would be an appropriate way to evaluate a model, but in
others, this is insufficient.</p>
<p>For example, suppose we want to use a classification model to predict the
likelihood of someone having a rare, but serious health condition.</p>
<p>If the condition is very rare (say it appears in 0.01% of the population), then
a model that always predicts false would have 99.99% accuracy, but the false
negatives could have large consequences.</p>
</div>
<div class="section" id="precision-and-recall">
<h3>Precision and Recall<a class="headerlink" href="#precision-and-recall" title="Permalink to this headline">¶</a></h3>
<p>In order to capture situations like that, data scientists often use two other
very common metrics:</p>
<ul class="simple">
<li><p><em>Precision</em>: The number of true positives over the number of positive
predictions. Precision tells us how often the model was correct when it
predicted true.</p></li>
<li><p><em>Recall</em>: The number of true positives over the number of actual positives.
Recall answers the question, “What fraction of the positives did we get
correct?”</p></li>
</ul>
<p>In the rare health condition example, you may prefer
a model with high recall (never misses an at-risk patient), even if the
precision is a bit low (sometimes you have false positives).</p>
<p>On the other hand, if your algorithm filters spam emails out of an inbox,
you may prefer a model with high precision so that when an email is
classified as spam, it is very likely to actually be spam (i.e. non-spam
messages don’t get sent to spam folder).</p>
<p>In many settings, both precision and recall are equally important and a
compound metric known as the F1-score is used:</p>
<div class="math notranslate nohighlight">
\[
F1 = 2 \frac{\text{precision} \cdot \text{recall}}{\text{precision} + \text{recall}}
\]</div>
<p>The F1 score is bounded between 0 and 1. It will only achieve a value of 1 if
both precision and recall are exactly 1.</p>
<p>We can have scikit-learn produce a textual report with precision and recall.</p>
<p>Scikit-learn</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">report</span> <span class="o">=</span> <span class="n">metrics</span><span class="o">.</span><span class="n">classification_report</span><span class="p">(</span>
    <span class="n">y_train</span><span class="p">,</span> <span class="n">logistic_age_model</span><span class="o">.</span><span class="n">predict</span><span class="p">(</span><span class="n">X_train</span><span class="p">),</span>
    <span class="n">target_names</span><span class="o">=</span><span class="p">[</span><span class="s2">&quot;no recid&quot;</span><span class="p">,</span> <span class="s2">&quot;recid&quot;</span><span class="p">]</span>
<span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="n">report</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>              precision    recall  f1-score   support

    no recid       0.67      0.73      0.69      2940
       recid       0.64      0.57      0.60      2470

    accuracy                           0.65      5410
   macro avg       0.65      0.65      0.65      5410
weighted avg       0.65      0.65      0.65      5410
</pre></div>
</div>
</div>
</div>
</div>
<div class="section" id="roc-and-auc">
<h3>ROC and AUC<a class="headerlink" href="#roc-and-auc" title="Permalink to this headline">¶</a></h3>
<p>For classification algorithms, there is a tradeoff between precision and recall.</p>
<p>Let’s illustrate this point in the context of the logistic regression model.</p>
<p>The output of a logistic regression is a probability of an event or label.</p>
<p>To obtain a definite prediction from the algorithm, the modeler would
first select a threshold parameter <span class="math notranslate nohighlight">\(p\)</span> such that all model outputs above the
threshold are given the label of true.</p>
<p>As this <span class="math notranslate nohighlight">\(p\)</span> increases, the model must be relatively more confident before
assigning a label of true.</p>
<p>In this case, the model’s precision will increase (very confident when applying
true label), but the recall will suffer (will apply false to some true cases
that had a model output just below the raised threshold).</p>
<p>Machine learning practitioners have adapted a way to help us visualize
this tradeoff.</p>
<p>The visualization technique is known as the receiver operating characteristic
– or more commonly used ROC – curve <a class="footnote-reference brackets" href="#roc" id="id1">1</a>.</p>
<p>To understand this curve, consider two extremes choices for <span class="math notranslate nohighlight">\(p\)</span>:</p>
<ul class="simple">
<li><p>When <span class="math notranslate nohighlight">\(p=1\)</span>, we will (almost surely) never predict any observation to
have a label 1. In this case, the false positive rate will be equal to 0, as
will the true positive rate.</p></li>
<li><p>When <span class="math notranslate nohighlight">\(p=0\)</span>, we will predict that all observations always have a label
of 1. The false positive rate and true positive rates will be equal to 1.</p></li>
</ul>
<p>The <em>ROC curve</em> traces the relationship between the false positive rate (on
the x axis) and the true positive rate (on the y axis) as the probability
threshold <span class="math notranslate nohighlight">\(p\)</span> is changed.</p>
<p>Below, we define a function that uses scikit-learn to compute the true positive
rate and false positive rates. Then we plot these rates against
each other.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="k">def</span> <span class="nf">plot_roc</span><span class="p">(</span><span class="n">mod</span><span class="p">,</span> <span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">):</span>
    <span class="c1"># predicted_probs is an N x 2 array, where N is number of observations</span>
    <span class="c1"># and 2 is number of classes</span>
    <span class="n">predicted_probs</span> <span class="o">=</span> <span class="n">mod</span><span class="o">.</span><span class="n">predict_proba</span><span class="p">(</span><span class="n">X_test</span><span class="p">)</span>

    <span class="c1"># keep the second column, for label=1</span>
    <span class="n">predicted_prob1</span> <span class="o">=</span> <span class="n">predicted_probs</span><span class="p">[:,</span> <span class="mi">1</span><span class="p">]</span>

    <span class="n">fpr</span><span class="p">,</span> <span class="n">tpr</span><span class="p">,</span> <span class="n">_</span> <span class="o">=</span> <span class="n">metrics</span><span class="o">.</span><span class="n">roc_curve</span><span class="p">(</span><span class="n">y_test</span><span class="p">,</span> <span class="n">predicted_prob1</span><span class="p">)</span>

    <span class="c1"># Plot ROC curve</span>
    <span class="n">fig</span><span class="p">,</span> <span class="n">ax</span> <span class="o">=</span> <span class="n">plt</span><span class="o">.</span><span class="n">subplots</span><span class="p">()</span>
    <span class="n">ax</span><span class="o">.</span><span class="n">plot</span><span class="p">([</span><span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">],</span> <span class="p">[</span><span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">],</span> <span class="s2">&quot;k--&quot;</span><span class="p">)</span>
    <span class="n">ax</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">fpr</span><span class="p">,</span> <span class="n">tpr</span><span class="p">)</span>
    <span class="n">ax</span><span class="o">.</span><span class="n">set_xlabel</span><span class="p">(</span><span class="s2">&quot;False Positive Rate&quot;</span><span class="p">)</span>
    <span class="n">ax</span><span class="o">.</span><span class="n">set_ylabel</span><span class="p">(</span><span class="s2">&quot;True Positive Rate&quot;</span><span class="p">)</span>
    <span class="n">ax</span><span class="o">.</span><span class="n">set_title</span><span class="p">(</span><span class="s2">&quot;ROC Curve&quot;</span><span class="p">)</span>

<span class="n">plot_roc</span><span class="p">(</span><span class="n">logistic_age_model</span><span class="p">,</span> <span class="n">X_test</span><span class="p">,</span> <span class="n">y_test</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<img alt="../_images/classification_15_0.png" src="../_images/classification_15_0.png" />
</div>
</div>
<p>We can use the ROC curve to determine the optimal threshold value.</p>
<p>Since the output of our recidivism application model could
potentially inform judicial decisions that impact the lives of individuals, we
should be careful when considering a threshold value with low false
positive rate vs high recall (low false negative rate).</p>
<p>We may choose to err on the side of low false negative rate so that when the model
predicts recidivism, recidivism will likely occur – in other words,
we would favor a high true positive rate even if the false positive rate is
higher.</p>
<div class="admonition-exercise admonition" id="app-cls-dir3">
<p class="admonition-title">Exercise</p>
<p>See exercise 3 in the <a class="reference internal" href="#app-cls-ex"><span class="std std-ref">exercise list</span></a>.</p>
</div>
<p>The ROC curve can also be used to do hyper-parameter selection for the model’s
parameters.</p>
<p>To see how, consider a model with an ROC curve that has a single point at (0, 1)
– meaning the true positive rate is 1 and false positive rate is zero or
that the model has 100% accuracy.</p>
<p>Notice that integrating to obtain the area under the ROC curve returns
a value of 1 for the perfect model.</p>
<p>The area under any other ROC curve would be less than 1.</p>
<p>Thus, we could use the area under the curve (abbreviated AUC) as an objective
metric in cross-validation.</p>
<p>Let’s see an example.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">predicted_prob1</span> <span class="o">=</span> <span class="n">logistic_age_model</span><span class="o">.</span><span class="n">predict_proba</span><span class="p">(</span><span class="n">X</span><span class="p">)[:,</span> <span class="mi">1</span><span class="p">]</span>
<span class="n">auc</span> <span class="o">=</span> <span class="n">metrics</span><span class="o">.</span><span class="n">roc_auc_score</span><span class="p">(</span><span class="n">y</span><span class="p">,</span> <span class="n">predicted_prob1</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Initial AUC value is </span><span class="si">{</span><span class="n">auc</span><span class="si">:</span><span class="s2">.4f</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>

<span class="c1"># help(linear_model.LogisticRegression)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Initial AUC value is 0.7057
</pre></div>
</div>
</div>
</div>
<div class="admonition-exercise admonition" id="app-cls-dir4">
<p class="admonition-title">Exercise</p>
<p>See exercise 4 in the <a class="reference internal" href="#app-cls-ex"><span class="std std-ref">exercise list</span></a>.</p>
</div>
</div>
</div>
<div class="section" id="neural-network-classifiers">
<h2>Neural Network Classifiers<a class="headerlink" href="#neural-network-classifiers" title="Permalink to this headline">¶</a></h2>
<p>The final classifier we will visit today is a neural-network classifier, using
the multi-layer perceptron network architecture.</p>
<p>Recall from the <a class="reference internal" href="regression.html"><span class="doc">regression</span></a> chapter that a multi-layer
perceptron is comprised of a series of nested linear regressions separated by
non-linear activation functions.</p>
<p>The number of neurons (size of weight matrices and bias vectors) in each layer
were hyperparameters that could be chosen by modeler, but for regression, the last
layer had to have exactly one neuron which represented the single regression
target.</p>
<p>To use the MLP for classification tasks, we need to make three adjustments:</p>
<ol class="simple">
<li><p>Construct a final layer with <span class="math notranslate nohighlight">\(N\)</span> neurons instead of 1, where <span class="math notranslate nohighlight">\(N\)</span> is the number of classes in the classification task.</p></li>
<li><p>Apply a <em>softmax</em> function on the network output.</p></li>
<li><p>Use the cross-entropy loss function instead of the MSE to optimize network weights and biases.</p></li>
</ol>
<p>The softmax function applied to a vector <span class="math notranslate nohighlight">\(x \in \mathbb{R}^N\)</span> is computed as</p>
<div class="math notranslate nohighlight">
\[
\sigma(x)_i = \frac{e^{x_i}}{\sum_{j=1}^{N} e^{x_j}}
\]</div>
<p>In words, the softmax function is computed by exponentiating all the values,
then dividing by the sum of exponentiated values.</p>
<p>The output of the softmax function is a probability distribution (all
non-negative and sum to 1) weighted by the relative value of the input values.</p>
<p>Finally, the cross entropy loss function for <span class="math notranslate nohighlight">\(M\)</span> observations <span class="math notranslate nohighlight">\(y\)</span>, with associated softmax vectors <span class="math notranslate nohighlight">\(z\)</span> is</p>
<div class="math notranslate nohighlight">
\[
-\frac{1}{M} \sum_{j=1}^M \sum_{i=1}^N 1_{y_j = i} log\left(z_{i,j}\right)
\]</div>
<p>where <span class="math notranslate nohighlight">\(1_{y_j = i}\)</span> is an indicator variable with the value of 1 if
the observed class was equal to <span class="math notranslate nohighlight">\(i\)</span> for the <span class="math notranslate nohighlight">\(j\)</span> th observation, 0
otherwise.</p>
<p>All the same tradeoffs we saw when we used the multi-layer perceptron for
regression will apply for classification tasks.</p>
<p>This includes positives like automated-feature enginnering and theoretically unlimited flexibility.</p>
<p>It also includes potential negatives, such as a risk of overfitting, high
computational expenses compared to many classification algorithms, and lack of
interpretability.</p>
<p>For a more detailed discussion, review the <a class="reference internal" href="regression.html"><span class="doc">regression lecture</span></a>.</p>
<div class="admonition-exercise admonition" id="app-cls-dir5">
<p class="admonition-title">Exercise</p>
<p>See exercise 5 in the <a class="reference internal" href="#app-cls-ex"><span class="std std-ref">exercise list</span></a>.</p>
</div>
<div class="section" id="aside-neural-network-toolboxes">
<h3>Aside: Neural Network Toolboxes<a class="headerlink" href="#aside-neural-network-toolboxes" title="Permalink to this headline">¶</a></h3>
<p>Thus far, we have been using the routines in scikit-learn’s <code class="docutils literal notranslate"><span class="pre">neural_network</span></code> package.</p>
<p>These are great for learning and exploratory analysis, as we have been doing,
but are rarely used in production or real-world settings.</p>
<p>Why? 1) The scikit-learn routines do not leverage modern
hardware like GPUs, so performance is likely much slower than it could be.
2) The routines only provide implementations of the most basic deep neural networks.</p>
<p>If you were to use neural networks in mission-critical situations, you would
want to use modern neural network libraries such as Google’s <a class="reference external" href="https://www.tensorflow.org/">tensorflow</a>,
Facebook’s <a class="reference external" href="https://pytorch.org/">pytorch</a>, the Amazon-supported <a class="reference external" href="https://mxnet.apache.org/">MXNet</a>, or
<a class="reference external" href="https://www.fast.ai/">fastai</a>.</p>
<p>Each of these toolkits has its own relative strengths and weaknesses, but we’ve
seen tensorflow and pytorch used the most.</p>
<p>Thankfully, they all support Python as either the only or the primary point of
access, so you will be well-prepared to start using them.</p>
</div>
</div>
<div class="section" id="application-predicting-us-recessions">
<h2>Application: Predicting US Recessions<a class="headerlink" href="#application-predicting-us-recessions" title="Permalink to this headline">¶</a></h2>
<p>Let’s apply our new classification algorithm knowledge and use
<a class="reference external" href="https://www.investopedia.com/terms/l/leadingindicator.asp">leading indicators</a>
to predict recessions in the US economy.</p>
<p>A leading indicator is a variable that moves or changes before the rest
of the economy.</p>
<p>Many different leading indicators have been proposed – we’ll use a few of them.</p>
<p>We won’t explicitly prove that these variables are actually leading indicators,
but will show a plot of each variables that lets us
visually inspect the hypothesis.</p>
<div class="section" id="data-prep">
<h3>Data Prep<a class="headerlink" href="#data-prep" title="Permalink to this headline">¶</a></h3>
<p>Let’s first gather the data from FRED.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">start</span> <span class="o">=</span> <span class="s2">&quot;1974-01-01&quot;</span>
<span class="n">end</span> <span class="o">=</span> <span class="n">datetime</span><span class="o">.</span><span class="n">date</span><span class="o">.</span><span class="n">today</span><span class="p">()</span>

<span class="k">def</span> <span class="nf">pct_change_on_last_year</span><span class="p">(</span><span class="n">df</span><span class="p">):</span>
    <span class="s2">&quot;compute pct_change on previous year, assuming quarterly&quot;</span>
    <span class="k">return</span> <span class="p">(</span><span class="n">df</span> <span class="o">-</span> <span class="n">df</span><span class="o">.</span><span class="n">shift</span><span class="p">(</span><span class="mi">4</span><span class="p">))</span><span class="o">/</span><span class="n">df</span><span class="o">.</span><span class="n">shift</span><span class="p">(</span><span class="mi">4</span><span class="p">)</span>

<span class="k">def</span> <span class="nf">get_indicators_from_fred</span><span class="p">(</span><span class="n">start</span><span class="o">=</span><span class="n">start</span><span class="p">,</span> <span class="n">end</span><span class="o">=</span><span class="n">end</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Fetch quarterly data on 6 leading indicators from time period start:end</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="c1"># yield curve, unemployment, change in inventory, new private housing permits</span>
    <span class="n">yc_unemp_inv_permit</span> <span class="o">=</span> <span class="p">(</span>
        <span class="n">web</span><span class="o">.</span><span class="n">DataReader</span><span class="p">([</span><span class="s2">&quot;T10Y2Y&quot;</span><span class="p">,</span> <span class="s2">&quot;UNRATE&quot;</span><span class="p">,</span> <span class="s2">&quot;CBIC1&quot;</span><span class="p">,</span> <span class="s2">&quot;PERMIT&quot;</span><span class="p">],</span> <span class="s2">&quot;fred&quot;</span><span class="p">,</span> <span class="n">start</span><span class="p">,</span> <span class="n">end</span><span class="p">)</span>
        <span class="o">.</span><span class="n">resample</span><span class="p">(</span><span class="s2">&quot;QS&quot;</span><span class="p">)</span>
        <span class="o">.</span><span class="n">mean</span><span class="p">()</span>
    <span class="p">)</span>

    <span class="c1"># percent change in housing prices and retail sales</span>
    <span class="n">hpi_retail</span> <span class="o">=</span> <span class="p">(</span>
        <span class="n">web</span><span class="o">.</span><span class="n">DataReader</span><span class="p">([</span><span class="s2">&quot;USSTHPI&quot;</span><span class="p">,</span> <span class="s2">&quot;SLRTTO01USQ661S&quot;</span><span class="p">],</span> <span class="s2">&quot;fred&quot;</span><span class="p">,</span> <span class="n">start</span><span class="p">,</span> <span class="n">end</span><span class="p">)</span>
        <span class="o">.</span><span class="n">resample</span><span class="p">(</span><span class="s2">&quot;QS&quot;</span><span class="p">)</span>  <span class="c1"># already quarterly, adjusting so index is same</span>
        <span class="o">.</span><span class="n">mean</span><span class="p">()</span>
        <span class="o">.</span><span class="n">pipe</span><span class="p">(</span><span class="n">pct_change_on_last_year</span><span class="p">)</span>
        <span class="o">.</span><span class="n">dropna</span><span class="p">()</span>
    <span class="p">)</span>

    <span class="n">indicators</span> <span class="o">=</span> <span class="p">(</span>
        <span class="n">yc_unemp_inv_permit</span>
        <span class="o">.</span><span class="n">join</span><span class="p">(</span><span class="n">hpi_retail</span><span class="p">)</span>
        <span class="o">.</span><span class="n">dropna</span><span class="p">()</span>
        <span class="o">.</span><span class="n">rename</span><span class="p">(</span><span class="n">columns</span><span class="o">=</span><span class="nb">dict</span><span class="p">(</span>
            <span class="n">USSTHPI</span><span class="o">=</span><span class="s2">&quot;pct_change_hpi&quot;</span><span class="p">,</span>
            <span class="n">T10Y2Y</span><span class="o">=</span><span class="s2">&quot;yield_curve&quot;</span><span class="p">,</span>
            <span class="n">UNRATE</span><span class="o">=</span><span class="s2">&quot;unemp&quot;</span><span class="p">,</span>
            <span class="n">CBIC1</span><span class="o">=</span><span class="s2">&quot;inventory&quot;</span><span class="p">,</span>
            <span class="n">SLRTTO01USQ661S</span><span class="o">=</span><span class="s2">&quot;retail_sales&quot;</span><span class="p">,</span>
            <span class="n">PERMIT</span><span class="o">=</span><span class="s2">&quot;house_permits&quot;</span>
        <span class="p">))</span>
    <span class="p">)</span>

    <span class="k">return</span> <span class="n">indicators</span>

<span class="n">indicators</span> <span class="o">=</span> <span class="n">get_indicators_from_fred</span><span class="p">()</span>

<span class="n">indicators</span><span class="o">.</span><span class="n">head</span><span class="p">()</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_html"><div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
</style>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>yield_curve</th>
      <th>unemp</th>
      <th>inventory</th>
      <th>house_permits</th>
      <th>pct_change_hpi</th>
      <th>retail_sales</th>
    </tr>
    <tr>
      <th>DATE</th>
      <th></th>
      <th></th>
      <th></th>
      <th></th>
      <th></th>
      <th></th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>1976-04-01</th>
      <td>0.801364</td>
      <td>7.566667</td>
      <td>61.087</td>
      <td>1171.333333</td>
      <td>0.075583</td>
      <td>0.056936</td>
    </tr>
    <tr>
      <th>1976-07-01</th>
      <td>1.099688</td>
      <td>7.733333</td>
      <td>55.190</td>
      <td>1345.000000</td>
      <td>0.087582</td>
      <td>0.037207</td>
    </tr>
    <tr>
      <th>1976-10-01</th>
      <td>1.467377</td>
      <td>7.766667</td>
      <td>20.163</td>
      <td>1489.000000</td>
      <td>0.081407</td>
      <td>0.047523</td>
    </tr>
    <tr>
      <th>1977-01-01</th>
      <td>1.332222</td>
      <td>7.500000</td>
      <td>34.343</td>
      <td>1562.000000</td>
      <td>0.104895</td>
      <td>0.037939</td>
    </tr>
    <tr>
      <th>1977-04-01</th>
      <td>1.248254</td>
      <td>7.133333</td>
      <td>51.245</td>
      <td>1693.333333</td>
      <td>0.110296</td>
      <td>0.032931</td>
    </tr>
  </tbody>
</table>
</div></div></div>
</div>
<p>Now, we also need data on recessions.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="k">def</span> <span class="nf">get_recession_data</span><span class="p">():</span>
    <span class="n">recession</span> <span class="o">=</span> <span class="p">(</span>
        <span class="n">web</span><span class="o">.</span><span class="n">DataReader</span><span class="p">([</span><span class="s2">&quot;USRECQ&quot;</span><span class="p">],</span> <span class="s2">&quot;fred&quot;</span><span class="p">,</span> <span class="n">start</span><span class="p">,</span> <span class="n">end</span><span class="p">)</span>
        <span class="o">.</span><span class="n">rename</span><span class="p">(</span><span class="n">columns</span><span class="o">=</span><span class="nb">dict</span><span class="p">(</span><span class="n">USRECQ</span><span class="o">=</span><span class="s2">&quot;recession&quot;</span><span class="p">))</span>
        <span class="p">[</span><span class="s2">&quot;recession&quot;</span><span class="p">]</span>
    <span class="p">)</span>

    <span class="c1"># extract start and end date for each recession</span>
    <span class="n">start_dates</span> <span class="o">=</span> <span class="n">recession</span><span class="o">.</span><span class="n">loc</span><span class="p">[</span><span class="n">recession</span><span class="o">.</span><span class="n">diff</span><span class="p">()</span> <span class="o">&gt;</span> <span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">index</span><span class="o">.</span><span class="n">tolist</span><span class="p">()</span>
    <span class="k">if</span> <span class="n">recession</span><span class="o">.</span><span class="n">iloc</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span> <span class="o">&gt;</span> <span class="mi">0</span><span class="p">:</span>
        <span class="n">start_dates</span> <span class="o">=</span> <span class="p">[</span><span class="n">recession</span><span class="o">.</span><span class="n">index</span><span class="p">[</span><span class="mi">0</span><span class="p">]]</span> <span class="o">+</span> <span class="n">start_dates</span>

    <span class="n">end_dates</span> <span class="o">=</span> <span class="n">recession</span><span class="o">.</span><span class="n">loc</span><span class="p">[</span><span class="n">recession</span><span class="o">.</span><span class="n">diff</span><span class="p">()</span> <span class="o">&lt;</span> <span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">index</span><span class="o">.</span><span class="n">tolist</span><span class="p">()</span>

    <span class="k">if</span> <span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">start_dates</span><span class="p">)</span> <span class="o">!=</span> <span class="nb">len</span><span class="p">(</span><span class="n">end_dates</span><span class="p">))</span> <span class="ow">and</span> <span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">start_dates</span><span class="p">)</span> <span class="o">!=</span> <span class="nb">len</span><span class="p">(</span><span class="n">end_dates</span><span class="p">)</span> <span class="o">+</span> <span class="mi">1</span><span class="p">):</span>
        <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="s2">&quot;Need to have same number of start/end dates!&quot;</span><span class="p">)</span>

    <span class="k">return</span> <span class="n">recession</span><span class="p">,</span> <span class="n">start_dates</span><span class="p">,</span> <span class="n">end_dates</span>

<span class="n">recession</span><span class="p">,</span> <span class="n">start_dates</span><span class="p">,</span> <span class="n">end_dates</span> <span class="o">=</span> <span class="n">get_recession_data</span><span class="p">()</span>
</pre></div>
</div>
</div>
</div>
<p>Now, let’s take a look at the data we have.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="k">def</span> <span class="nf">add_recession_bands</span><span class="p">(</span><span class="n">ax</span><span class="p">):</span>
    <span class="k">for</span> <span class="n">s</span><span class="p">,</span> <span class="n">e</span> <span class="ow">in</span> <span class="nb">zip</span><span class="p">(</span><span class="n">start_dates</span><span class="p">,</span> <span class="n">end_dates</span><span class="p">):</span>
        <span class="n">ax</span><span class="o">.</span><span class="n">axvspan</span><span class="p">(</span><span class="n">s</span><span class="p">,</span> <span class="n">e</span><span class="p">,</span> <span class="n">color</span><span class="o">=</span><span class="s2">&quot;grey&quot;</span><span class="p">,</span> <span class="n">alpha</span><span class="o">=</span><span class="mf">0.2</span><span class="p">)</span>

<span class="n">axs</span> <span class="o">=</span> <span class="n">indicators</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">subplots</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">8</span><span class="p">,</span> <span class="mi">6</span><span class="p">),</span> <span class="n">layout</span><span class="o">=</span><span class="p">(</span><span class="mi">3</span><span class="p">,</span> <span class="mi">2</span><span class="p">),</span> <span class="n">legend</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span>

<span class="k">for</span> <span class="n">i</span><span class="p">,</span> <span class="n">ax</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="n">axs</span><span class="o">.</span><span class="n">flatten</span><span class="p">()):</span>
    <span class="n">add_recession_bands</span><span class="p">(</span><span class="n">ax</span><span class="p">)</span>
    <span class="n">ax</span><span class="o">.</span><span class="n">set_title</span><span class="p">(</span><span class="nb">list</span><span class="p">(</span><span class="n">indicators</span><span class="p">)[</span><span class="n">i</span><span class="p">])</span>

<span class="n">fig</span> <span class="o">=</span> <span class="n">axs</span><span class="p">[</span><span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">get_figure</span><span class="p">()</span>
<span class="n">fig</span><span class="o">.</span><span class="n">tight_layout</span><span class="p">();</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<img alt="../_images/classification_23_0.png" src="../_images/classification_23_0.png" />
</div>
</div>
<p>For each of the chosen variables, you can see that the leading indicator
has a distinct move in periods leading up to a recession (noted by the grey bands in background).</p>
<div class="admonition-exercise admonition" id="app-cls-dir6">
<p class="admonition-title">Exercise</p>
<p>See exercise 6 in the <a class="reference internal" href="#app-cls-ex"><span class="std std-ref">exercise list</span></a>.</p>
</div>
</div>
<div class="section" id="how-many-leads">
<h3>How Many leads?<a class="headerlink" href="#how-many-leads" title="Permalink to this headline">¶</a></h3>
<p>If the variables we have chosen truly are leading indicators, we should be able
to use leading values of the variables to predict current or future recessions.</p>
<p>A natural question is: how many leads should we include?</p>
<p>Let’s explore that question by looking at many different sets of leads.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="k">def</span> <span class="nf">make_train_data</span><span class="p">(</span><span class="n">indicators</span><span class="p">,</span> <span class="n">rec</span><span class="p">,</span> <span class="n">nlead</span><span class="o">=</span><span class="mi">4</span><span class="p">):</span>
    <span class="k">return</span> <span class="n">indicators</span><span class="o">.</span><span class="n">join</span><span class="p">(</span><span class="n">rec</span><span class="o">.</span><span class="n">shift</span><span class="p">(</span><span class="n">nlead</span><span class="p">))</span><span class="o">.</span><span class="n">dropna</span><span class="p">()</span>

<span class="k">def</span> <span class="nf">fit_for_nlead</span><span class="p">(</span><span class="n">ind</span><span class="p">,</span> <span class="n">rec</span><span class="p">,</span> <span class="n">nlead</span><span class="p">,</span> <span class="n">mod</span><span class="p">):</span>
    <span class="n">df</span> <span class="o">=</span> <span class="n">make_train_data</span><span class="p">(</span><span class="n">ind</span><span class="p">,</span> <span class="n">rec</span><span class="p">,</span> <span class="n">nlead</span><span class="p">)</span>
    <span class="n">X</span> <span class="o">=</span> <span class="n">df</span><span class="o">.</span><span class="n">drop</span><span class="p">([</span><span class="s2">&quot;recession&quot;</span><span class="p">],</span> <span class="n">axis</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span><span class="o">.</span><span class="n">copy</span><span class="p">()</span>
    <span class="n">y</span> <span class="o">=</span> <span class="n">df</span><span class="p">[</span><span class="s2">&quot;recession&quot;</span><span class="p">]</span><span class="o">.</span><span class="n">copy</span><span class="p">()</span>

    <span class="n">X_train</span><span class="p">,</span> <span class="n">X_test</span><span class="p">,</span> <span class="n">y_train</span><span class="p">,</span> <span class="n">y_test</span> <span class="o">=</span> <span class="n">model_selection</span><span class="o">.</span><span class="n">train_test_split</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">)</span>

    <span class="n">mod</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X_train</span><span class="p">,</span> <span class="n">y_train</span><span class="p">)</span>
    <span class="n">cmat</span> <span class="o">=</span> <span class="n">metrics</span><span class="o">.</span><span class="n">confusion_matrix</span><span class="p">(</span><span class="n">y_test</span><span class="p">,</span> <span class="n">mod</span><span class="o">.</span><span class="n">predict</span><span class="p">(</span><span class="n">X_test</span><span class="p">))</span>
    <span class="k">return</span> <span class="n">cmat</span>

<span class="n">mod</span> <span class="o">=</span> <span class="n">pipeline</span><span class="o">.</span><span class="n">make_pipeline</span><span class="p">(</span>
    <span class="n">preprocessing</span><span class="o">.</span><span class="n">StandardScaler</span><span class="p">(),</span>
    <span class="n">linear_model</span><span class="o">.</span><span class="n">LogisticRegression</span><span class="p">(</span><span class="n">solver</span><span class="o">=</span><span class="s2">&quot;lbfgs&quot;</span><span class="p">)</span>
<span class="p">)</span>

<span class="n">cmats</span> <span class="o">=</span> <span class="nb">dict</span><span class="p">()</span>
<span class="k">for</span> <span class="n">nlead</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">11</span><span class="p">):</span>
    <span class="n">cmats</span><span class="p">[</span><span class="n">nlead</span><span class="p">]</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">zeros</span><span class="p">((</span><span class="mi">2</span><span class="p">,</span> <span class="mi">2</span><span class="p">))</span>
    <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;starting for </span><span class="si">{</span><span class="n">nlead</span><span class="si">}</span><span class="s2"> leads&quot;</span><span class="p">)</span>
    <span class="k">for</span> <span class="n">rep</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">200</span><span class="p">):</span>
        <span class="n">cmats</span><span class="p">[</span><span class="n">nlead</span><span class="p">]</span> <span class="o">+=</span> <span class="n">fit_for_nlead</span><span class="p">(</span><span class="n">indicators</span><span class="p">,</span> <span class="n">recession</span><span class="p">,</span> <span class="n">nlead</span><span class="p">,</span> <span class="n">mod</span><span class="p">)</span>

    <span class="n">cmats</span><span class="p">[</span><span class="n">nlead</span><span class="p">]</span> <span class="o">=</span> <span class="n">cmats</span><span class="p">[</span><span class="n">nlead</span><span class="p">]</span> <span class="o">/</span> <span class="mi">200</span>

<span class="k">for</span> <span class="n">k</span><span class="p">,</span> <span class="n">v</span> <span class="ow">in</span> <span class="n">cmats</span><span class="o">.</span><span class="n">items</span><span class="p">():</span>
    <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;</span><span class="se">\n\n</span><span class="s2">The average confusion matrix for </span><span class="si">{</span><span class="n">k</span><span class="si">}</span><span class="s2"> lag(s) was:</span><span class="se">\n</span><span class="s2"> </span><span class="si">{</span><span class="n">v</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>starting for 1 leads
</pre></div>
</div>
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>starting for 2 leads
</pre></div>
</div>
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>starting for 3 leads
</pre></div>
</div>
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>starting for 4 leads
</pre></div>
</div>
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>starting for 5 leads
</pre></div>
</div>
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>starting for 6 leads
</pre></div>
</div>
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>starting for 7 leads
</pre></div>
</div>
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>starting for 8 leads
</pre></div>
</div>
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>starting for 9 leads
</pre></div>
</div>
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>starting for 10 leads
</pre></div>
</div>
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>The average confusion matrix for 1 lag(s) was:
 [[37.58   0.6  ]
 [ 1.555  3.265]]


The average confusion matrix for 2 lag(s) was:
 [[37.94   0.715]
 [ 2.33   2.015]]


The average confusion matrix for 3 lag(s) was:
 [[37.835  0.89 ]
 [ 2.795  2.125]]


The average confusion matrix for 4 lag(s) was:
 [[38.2   0.41]
 [ 2.45  1.94]]


The average confusion matrix for 5 lag(s) was:
 [[37.21   0.915]
 [ 3.49   1.385]]


The average confusion matrix for 6 lag(s) was:
 [[36.595  1.33 ]
 [ 3.695  1.38 ]]


The average confusion matrix for 7 lag(s) was:
 [[36.3    1.325]
 [ 3.555  1.82 ]]


The average confusion matrix for 8 lag(s) was:
 [[35.955  1.2  ]
 [ 3.895  1.95 ]]


The average confusion matrix for 9 lag(s) was:
 [[35.465  1.575]
 [ 4.42   1.54 ]]


The average confusion matrix for 10 lag(s) was:
 [[34.645  1.495]
 [ 4.865  0.995]]
</pre></div>
</div>
</div>
</div>
<p>From the averaged confusion matrices reported above, we see that the model with
only one leading period was the most accurate.</p>
<p>After that was the model with 4 leading quarters.</p>
<p>Depending on the application, we might favor a model with higher accuracy or
one that gives us more time to prepare (the 4 quarter model).</p>
<p>Why did the 1-lead and 4-lead models perform better than models with
another number of leads? Perhaps because different variables start moving a
different number of periods before the recession hits.</p>
<p>The exercise below asks you to explore this idea.</p>
<div class="admonition-exercise admonition" id="app-cls-dir7">
<p class="admonition-title">Exercise</p>
<p>See exercise 7 in the <a class="reference internal" href="#app-cls-ex"><span class="std std-ref">exercise list</span></a>.</p>
</div>
<div class="admonition-exercise admonition" id="app-cls-dir8">
<p class="admonition-title">Exercise</p>
<p>See exercise 8 in the <a class="reference internal" href="#app-cls-ex"><span class="std std-ref">exercise list</span></a>.</p>
</div>
</div>
</div>
<div class="section" id="exercises">
<span id="app-cls-ex"></span><h2>Exercises<a class="headerlink" href="#exercises" title="Permalink to this headline">¶</a></h2>
<div class="section" id="exercise-1">
<h3>Exercise 1<a class="headerlink" href="#exercise-1" title="Permalink to this headline">¶</a></h3>
<p>Determine the level of this cutoff value. Recall that the COMPAS
score takes on integer values between 1 and 10, inclusive.</p>
<p>What happens to the cutoff level of the <code class="docutils literal notranslate"><span class="pre">decile_score</span></code> when you change
the classification threshold from 0.5 to 0.7? What about 0.3? Remember this
idea – we’ll come back to it soon.</p>
<p>(<a class="reference internal" href="#app-cls-dir1"><span class="std std-ref">back to text</span></a>)</p>
</div>
<div class="section" id="exercise-2">
<h3>Exercise 2<a class="headerlink" href="#exercise-2" title="Permalink to this headline">¶</a></h3>
<p>Experiment with different pairs of features to see which ones show the
clearest decision boundaries.</p>
<p>Feed different <code class="docutils literal notranslate"><span class="pre">X</span></code> DataFrames into the <code class="docutils literal notranslate"><span class="pre">fit_and_plot_decision_boundary</span></code> function above.</p>
<p>(<a class="reference internal" href="#app-cls-dir2"><span class="std std-ref">back to text</span></a>)</p>
</div>
<div class="section" id="exercise-3">
<h3>Exercise 3<a class="headerlink" href="#exercise-3" title="Permalink to this headline">¶</a></h3>
<p>Use the <code class="docutils literal notranslate"><span class="pre">metrics.roc_curve</span></code> function to determine an appropriate value
for the probability threshold, keeping in mind our preference for
high precision over high recall.</p>
<p>The third return value of <code class="docutils literal notranslate"><span class="pre">metrics.roc_curve</span></code> is an array of the
probability thresholds (<code class="docutils literal notranslate"><span class="pre">p</span></code>) used to compute each false positive rate and
true positive rate.</p>
<p>To do this problem, you may wish to do the following steps:</p>
<ul class="simple">
<li><p>Concoct objective function in terms of the <code class="docutils literal notranslate"><span class="pre">fpr</span></code> and <code class="docutils literal notranslate"><span class="pre">tpr</span></code>.</p></li>
<li><p>Evaluate the objective function using the <code class="docutils literal notranslate"><span class="pre">fpr</span></code> and <code class="docutils literal notranslate"><span class="pre">tpr</span></code> variables returned by the <code class="docutils literal notranslate"><span class="pre">metrics.roc_curve</span></code> function.</p></li>
<li><p>Use <code class="docutils literal notranslate"><span class="pre">np.argmin</span></code> to find the  <em>index</em> of the smallest value of the objective function.</p></li>
<li><p>Extract the value at the margin index from the probability threshold values array.</p></li>
</ul>
<div class="admonition hint">
<p class="admonition-title">Hint</p>
<p>If we cared about both precision and recall equally (we don’t here),
we might choose <code class="docutils literal notranslate"><span class="pre">(fpr</span> <span class="pre">-</span> <span class="pre">tpr)**2</span></code> as one objective function. With this
objective function, we would find the probability threshold value
that makes the false positive and true positive rates as equal as
possible.</p>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># your code here</span>
</pre></div>
</div>
</div>
</div>
<p>(<a class="reference internal" href="#app-cls-dir3"><span class="std std-ref">back to text</span></a>)</p>
</div>
<div class="section" id="exercise-4">
<h3>Exercise 4<a class="headerlink" href="#exercise-4" title="Permalink to this headline">¶</a></h3>
<p>The <code class="docutils literal notranslate"><span class="pre">LogisticRegression</span></code> class with default arguments implements the
regression including <code class="docutils literal notranslate"><span class="pre">l2</span></code> regularization (it penalizes coefficient
vectors with an l2-norm).</p>
<p>The regularization strength is controlled by a parameter <code class="docutils literal notranslate"><span class="pre">C</span></code> that is
passed to the <code class="docutils literal notranslate"><span class="pre">LogisticRegression</span></code> constructor.</p>
<p>Smaller values of <code class="docutils literal notranslate"><span class="pre">C</span></code> lead to stronger regularization.</p>
<p>For example, <code class="docutils literal notranslate"><span class="pre">LogisticRegression(C=10)</span></code> would have weaker regularization
than <code class="docutils literal notranslate"><span class="pre">LogisticRegression(C=0.5)</span></code>.</p>
<p>Your task here is to use the <code class="docutils literal notranslate"><span class="pre">model_selection.cross_val_score</span></code> method to select an
optimal level for the regularization parameter <code class="docutils literal notranslate"><span class="pre">C</span></code>. The <code class="docutils literal notranslate"><span class="pre">scoring</span></code> argument should be set
to <code class="docutils literal notranslate"><span class="pre">roc_auc</span></code>.</p>
<p>Refer to the example in the <a class="reference internal" href="recidivism.html"><span class="doc">recidivism lecture</span></a> for how
to use <code class="docutils literal notranslate"><span class="pre">model_selection.cross_val_score</span></code>.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># your code here</span>
</pre></div>
</div>
</div>
</div>
<p>(<a class="reference internal" href="#app-cls-dir4"><span class="std std-ref">back to text</span></a>)</p>
</div>
<div class="section" id="exercise-5">
<h3>Exercise 5<a class="headerlink" href="#exercise-5" title="Permalink to this headline">¶</a></h3>
<p>Use a multi-layer perceptron in our recidivism example via the <code class="docutils literal notranslate"><span class="pre">neural_network.MLPClassifier</span></code> class.</p>
<p>Experiment with different inputs such as:</p>
<ul class="simple">
<li><p>The features to include</p></li>
<li><p>The number of layers and number of neurons in each layer</p></li>
<li><p>The l2 regularization parameter <code class="docutils literal notranslate"><span class="pre">alpha</span></code></p></li>
<li><p>The solver</p></li>
</ul>
<p>See if you can create a model that outperforms logistic regression.</p>
<p>Keep in mind other things, like the degree of overfitting and time required
to estimate the model parameters. How do these compare to logistic
regression?</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># your code here</span>
</pre></div>
</div>
</div>
</div>
<p>(<a class="reference internal" href="#app-cls-dir5"><span class="std std-ref">back to text</span></a>)</p>
</div>
<div class="section" id="exercise-6">
<h3>Exercise 6<a class="headerlink" href="#exercise-6" title="Permalink to this headline">¶</a></h3>
<p>Let’s pause here to take a few minutes and digest.</p>
<p>If the task is to use these leading indicators to predict a recession,
would high recall or high precision be more important for our model?</p>
<p>Would your answer change if you worked at the Federal Reserve?</p>
<p>What if you worked at a news company such as the Economist or the New York
Times?</p>
<p>(<a class="reference internal" href="#app-cls-dir6"><span class="std std-ref">back to text</span></a>)</p>
</div>
<div class="section" id="exercise-7">
<h3>Exercise 7<a class="headerlink" href="#exercise-7" title="Permalink to this headline">¶</a></h3>
<p>Extend the logic from the previous example and allow a different number
of leading periods for each variable.</p>
<p>How would you find the “optimal” number of leads for each variable? How
could you try to avoid overfitting?</p>
<p>Use <code class="docutils literal notranslate"><span class="pre">make_train_data_varying_leads</span></code> function below to construct your model.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="k">def</span> <span class="nf">make_train_data_varying_leads</span><span class="p">(</span><span class="n">indicators</span><span class="p">,</span> <span class="n">rec</span><span class="p">,</span> <span class="n">nlead</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Apply per-indicator leads to each indicator and join with recession data</span>

<span class="sd">    Parameters</span>
<span class="sd">    ----------</span>
<span class="sd">    indicators: pd.DataFrame</span>
<span class="sd">        A DataFrame with timestamps on index and leading indicators as columns</span>

<span class="sd">    rec: pd.Series</span>
<span class="sd">        A Series indicating if the US economy was in a recession each period</span>

<span class="sd">    nlead: dict</span>
<span class="sd">        A dictionary which maps a column name to a positive integer</span>
<span class="sd">        specifying how many periods to shift each indicator. Any</span>
<span class="sd">        indicator not given a key in this dictionary will not be</span>
<span class="sd">        included in the output DataFrame.</span>

<span class="sd">    Returns</span>
<span class="sd">    -------</span>
<span class="sd">    df: pd.DataFrame</span>
<span class="sd">        A DataFrame with the leads applied and merged with the recession</span>
<span class="sd">        indicator</span>

<span class="sd">    Example</span>
<span class="sd">    -------</span>

<span class="sd">    ```</span>
<span class="sd">    df = make_train_data_varying_leads(</span>
<span class="sd">        indicators,</span>
<span class="sd">        recession,</span>
<span class="sd">        nlead=dict(yield_curve=3, unemp=4)</span>
<span class="sd">    )</span>

<span class="sd">    df.shape[1]  # == 3 (yield_curve, unemp, recession))</span>
<span class="sd">    ```</span>

<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">cols</span> <span class="o">=</span> <span class="p">[]</span>
    <span class="k">for</span> <span class="n">col</span> <span class="ow">in</span> <span class="nb">list</span><span class="p">(</span><span class="n">indicators</span><span class="p">):</span>
        <span class="k">if</span> <span class="n">col</span> <span class="ow">in</span> <span class="n">nlead</span><span class="p">:</span>
            <span class="n">cols</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">indicators</span><span class="p">[</span><span class="n">col</span><span class="p">]</span><span class="o">.</span><span class="n">shift</span><span class="p">(</span><span class="o">-</span><span class="n">nlead</span><span class="p">[</span><span class="n">col</span><span class="p">]))</span>

    <span class="n">X</span> <span class="o">=</span> <span class="n">pd</span><span class="o">.</span><span class="n">concat</span><span class="p">(</span><span class="n">cols</span><span class="p">,</span> <span class="n">axis</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">X</span><span class="o">.</span><span class="n">join</span><span class="p">(</span><span class="n">rec</span><span class="p">)</span><span class="o">.</span><span class="n">dropna</span><span class="p">()</span>

<span class="c1"># your code here!</span>
</pre></div>
</div>
</div>
</div>
<p>(<a class="reference internal" href="#app-cls-dir7"><span class="std std-ref">back to text</span></a>)</p>
</div>
<div class="section" id="exercise-8">
<h3>Exercise 8<a class="headerlink" href="#exercise-8" title="Permalink to this headline">¶</a></h3>
<p>Experiment with different classifiers. Which ones perform better or worse?</p>
<p>How accurate can you become for each accuracy metric (accuracy, precision, and recall)?</p>
<p>(<a class="reference internal" href="#app-cls-dir8"><span class="std std-ref">back to text</span></a>)</p>
<hr class="footnotes docutils" />
<dl class="footnote brackets">
<dt class="label" id="roc"><span class="brackets"><a class="fn-backref" href="#id1">1</a></span></dt>
<dd><p>The name “receiver operating characteristic” comes from its origin;
during World War II, engineers used ROC curves to measure how well a radar signal
could be properly detected from noise (i.e. enemy aircraft vs. noise).</p>
</dd>
</dl>
</div>
</div>
</div>

    <script type="text/x-thebe-config">
    {
        requestKernel: true,
        binderOptions: {
            repo: "binder-examples/jupyter-stacks-datascience",
            ref: "master",
        },
        codeMirrorConfig: {
            theme: "abcdef",
            mode: "python"
        },
        kernelOptions: {
            kernelName: "python3",
            path: "./applications"
        },
        predefinedOutput: true
    }
    </script>
    <script>kernelName = 'python3'</script>

              </div>
              
        
        <div class='prev-next-bottom'>
            
    <a class='left-prev' id="prev-link" href="maps.html" title="previous page">Mapping in Python</a>
    <a class='right-next' id="next-link" href="working_with_text.html" title="next page">Working with Text</a>

        </div>
        
        </div>
    </div>
    <footer class="footer mt-5 mt-md-0">
    <div class="container">
      <p>
        
          By Chase Coleman, Spencer Lyon, and Jesse Perla<br/>
        
            &copy; Copyright 2020.<br/>
      </p>
    </div>
  </footer>
</main>


      </div>
    </div>
  
  <script src="../_static/js/index.1c5a1a01449ed65a7b51.js"></script>

  
  </body>
</html>